{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "508bf467-6c9e-4286-ac23-4f8872f82b95",
   "metadata": {},
   "source": [
    "# DATASETS CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d513e72-ad13-4f3c-ad3b-22efb2d082d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned_Customers.csv saved.\n",
      "Cleaned_Exchange_Rates.csv saved.\n",
      "Cleaned_Products.csv saved.\n",
      "Cleaned_Sales.csv saved.\n",
      "Cleaned_Stores.csv saved.\n",
      "All datasets cleaned successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to clean dataframe\n",
    "def clean_dataframe(df, date_columns=[]):\n",
    "    # Standardize column names: Replace spaces with underscores\n",
    "    df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "    # Handle date columns: Convert and format correctly\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Replace problematic values (NaN, NaT, empty strings) with None\n",
    "    df.replace({np.nan: None, \"nan\": None, \"NaT\": None, \"\": None}, inplace=True)\n",
    "    \n",
    "    # Convert all columns to appropriate types\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = df[col].apply(lambda x: None if pd.isna(x) or x == \"nan\" else x.strip() if isinstance(x, str) else x)\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Convert NaN values in numeric columns to None (MySQL NULL)\n",
    "    df = df.where(pd.notna(df), None)\n",
    "\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "# List of datasets with required cleaning steps\n",
    "datasets = {\n",
    "    \"Customers\": {\"drop_cols\": [\"State_Code\", \"Zip_Code\", \"Continent\"], \"date_cols\": [\"Birthday\"]},\n",
    "    \"Exchange_Rates\": {\"drop_cols\": [], \"date_cols\": [\"Date\"]},\n",
    "    \"Products\": {\"drop_cols\": [\"SubcategoryKey\"], \"date_cols\": []},\n",
    "    \"Sales\": {\"drop_cols\": [], \"date_cols\": [\"Order_Date\", \"Delivery_Date\"]},\n",
    "    \"Stores\": {\"drop_cols\": [], \"date_cols\": [\"Open_Date\"]}\n",
    "}\n",
    "\n",
    "# Process each dataset\n",
    "for name, config in datasets.items():\n",
    "    df = pd.read_csv(f\"{name}.csv\", encoding=\"ANSI\")\n",
    "    \n",
    "    # Replace spaces in column names before dropping any columns\n",
    "    df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "    # Drop specified columns if they exist\n",
    "    df.drop(columns=[col for col in config[\"drop_cols\"] if col in df.columns], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # Clean the dataframe\n",
    "    df = clean_dataframe(df, date_columns=config[\"date_cols\"])\n",
    "\n",
    "    # Additional cleaning for specific datasets\n",
    "    if name == \"Products\":\n",
    "        df[\"Unit_Cost_USD\"] = df[\"Unit_Cost_USD\"].str.replace(\"$\", \"\").str.replace(\",\", \"\").str.strip().astype(float)\n",
    "        df[\"Unit_Price_USD\"] = df[\"Unit_Price_USD\"].str.replace(\"$\", \"\").str.replace(\",\", \"\").str.strip().astype(float)\n",
    "    if name == \"Stores\":\n",
    "        df[\"Square_Meters\"] = df[\"Square_Meters\"].fillna(0)\n",
    "    if name == \"Sales\":\n",
    "        df.loc[:, \"Delivery_Date\"] = df[\"Delivery_Date\"].fillna(np.nan)  # Avoids chained assignment issue\n",
    "\n",
    "    # Save cleaned CSV\n",
    "    df.to_csv(f\"Cleaned_{name}.csv\", index=False)\n",
    "    print(f\"Cleaned_{name}.csv saved.\")\n",
    "\n",
    "print(\"All datasets cleaned successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93757ff-6701-47b8-a0d3-2eb44b37eea0",
   "metadata": {},
   "source": [
    "# MERGING ALL DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bac375c-b1cc-4cb0-8db5-8f0cbaea382f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved as 'Merged_Dataset.csv'\n",
      "   Order_Number  Line_Item  Order_Date Delivery_Date  CustomerKey  StoreKey  \\\n",
      "0        366000          1  2016-01-01          None       265598        10   \n",
      "1        366000          1  2016-01-01          None       265598        10   \n",
      "2        366000          1  2016-01-01          None       265598        10   \n",
      "3        366000          1  2016-01-01          None       265598        10   \n",
      "4        366000          1  2016-01-01          None       265598        10   \n",
      "\n",
      "   ProductKey  Quantity Currency_Code Gender  ...  Color Unit_Cost_USD  \\\n",
      "0        1304         1           CAD   Male  ...  White         31.27   \n",
      "1        1304         1           CAD   Male  ...  White         31.27   \n",
      "2        1304         1           CAD   Male  ...  White         31.27   \n",
      "3        1304         1           CAD   Male  ...  White         31.27   \n",
      "4        1304         1           CAD   Male  ...  White         31.27   \n",
      "\n",
      "  Unit_Price_USD                       Subcategory CategoryKey  \\\n",
      "0           68.0  Cameras & Camcorders Accessories           4   \n",
      "1           68.0  Cameras & Camcorders Accessories           4   \n",
      "2           68.0  Cameras & Camcorders Accessories           4   \n",
      "3           68.0  Cameras & Camcorders Accessories           4   \n",
      "4           68.0  Cameras & Camcorders Accessories           4   \n",
      "\n",
      "                 Category Square_Meters   Open_Date  Currency  Exchange  \n",
      "0  Cameras and camcorders        1210.0  2015-04-04       USD    1.0000  \n",
      "1  Cameras and camcorders        1210.0  2015-04-04       CAD    1.3884  \n",
      "2  Cameras and camcorders        1210.0  2015-04-04       AUD    1.3683  \n",
      "3  Cameras and camcorders        1210.0  2015-04-04       EUR    0.9185  \n",
      "4  Cameras and camcorders        1210.0  2015-04-04       GBP    0.6742  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "Columns: ['Order_Number', 'Line_Item', 'Order_Date', 'Delivery_Date', 'CustomerKey', 'StoreKey', 'ProductKey', 'Quantity', 'Currency_Code', 'Gender', 'Name', 'City', 'State', 'Country', 'Birthday', 'Product_Name', 'Brand', 'Color', 'Unit_Cost_USD', 'Unit_Price_USD', 'Subcategory', 'CategoryKey', 'Category', 'Square_Meters', 'Open_Date', 'Currency', 'Exchange']\n",
      "        Order_Number  Line_Item  Order_Date Delivery_Date  CustomerKey  \\\n",
      "0             366000          1  2016-01-01          None       265598   \n",
      "1             366000          1  2016-01-01          None       265598   \n",
      "2             366000          1  2016-01-01          None       265598   \n",
      "3             366000          1  2016-01-01          None       265598   \n",
      "4             366000          1  2016-01-01          None       265598   \n",
      "...              ...        ...         ...           ...          ...   \n",
      "314415       2243032          3  2021-02-20    2021-02-23       331277   \n",
      "314416       2243032          3  2021-02-20    2021-02-23       331277   \n",
      "314417       2243032          3  2021-02-20    2021-02-23       331277   \n",
      "314418       2243032          3  2021-02-20    2021-02-23       331277   \n",
      "314419       2243032          3  2021-02-20    2021-02-23       331277   \n",
      "\n",
      "        StoreKey  ProductKey  Quantity Currency_Code Gender              Name  \\\n",
      "0             10        1304         1           CAD   Male      Tyler Vaught   \n",
      "1             10        1304         1           CAD   Male      Tyler Vaught   \n",
      "2             10        1304         1           CAD   Male      Tyler Vaught   \n",
      "3             10        1304         1           CAD   Male      Tyler Vaught   \n",
      "4             10        1304         1           CAD   Male      Tyler Vaught   \n",
      "...          ...         ...       ...           ...    ...               ...   \n",
      "314415         0         464         7           CAD   Male  William Rochelle   \n",
      "314416         0         464         7           CAD   Male  William Rochelle   \n",
      "314417         0         464         7           CAD   Male  William Rochelle   \n",
      "314418         0         464         7           CAD   Male  William Rochelle   \n",
      "314419         0         464         7           CAD   Male  William Rochelle   \n",
      "\n",
      "           City    State Country    Birthday                     Product_Name  \\\n",
      "0        London  Ontario  Canada  1971-03-23  Contoso Lens Adapter M450 White   \n",
      "1        London  Ontario  Canada  1971-03-23  Contoso Lens Adapter M450 White   \n",
      "2        London  Ontario  Canada  1971-03-23  Contoso Lens Adapter M450 White   \n",
      "3        London  Ontario  Canada  1971-03-23  Contoso Lens Adapter M450 White   \n",
      "4        London  Ontario  Canada  1971-03-23  Contoso Lens Adapter M450 White   \n",
      "...         ...      ...     ...         ...                              ...   \n",
      "314415  Calgary  Alberta  Canada  1993-05-25     Proseware LCD22W M2001 Black   \n",
      "314416  Calgary  Alberta  Canada  1993-05-25     Proseware LCD22W M2001 Black   \n",
      "314417  Calgary  Alberta  Canada  1993-05-25     Proseware LCD22W M2001 Black   \n",
      "314418  Calgary  Alberta  Canada  1993-05-25     Proseware LCD22W M2001 Black   \n",
      "314419  Calgary  Alberta  Canada  1993-05-25     Proseware LCD22W M2001 Black   \n",
      "\n",
      "            Brand  Color  Unit_Cost_USD  Unit_Price_USD  \\\n",
      "0         Contoso  White          31.27            68.0   \n",
      "1         Contoso  White          31.27            68.0   \n",
      "2         Contoso  White          31.27            68.0   \n",
      "3         Contoso  White          31.27            68.0   \n",
      "4         Contoso  White          31.27            68.0   \n",
      "...           ...    ...            ...             ...   \n",
      "314415  Proseware  Black         224.97           679.0   \n",
      "314416  Proseware  Black         224.97           679.0   \n",
      "314417  Proseware  Black         224.97           679.0   \n",
      "314418  Proseware  Black         224.97           679.0   \n",
      "314419  Proseware  Black         224.97           679.0   \n",
      "\n",
      "                             Subcategory  CategoryKey                Category  \\\n",
      "0       Cameras & Camcorders Accessories            4  Cameras and camcorders   \n",
      "1       Cameras & Camcorders Accessories            4  Cameras and camcorders   \n",
      "2       Cameras & Camcorders Accessories            4  Cameras and camcorders   \n",
      "3       Cameras & Camcorders Accessories            4  Cameras and camcorders   \n",
      "4       Cameras & Camcorders Accessories            4  Cameras and camcorders   \n",
      "...                                  ...          ...                     ...   \n",
      "314415                          Monitors            3               Computers   \n",
      "314416                          Monitors            3               Computers   \n",
      "314417                          Monitors            3               Computers   \n",
      "314418                          Monitors            3               Computers   \n",
      "314419                          Monitors            3               Computers   \n",
      "\n",
      "        Square_Meters   Open_Date Currency  Exchange  \n",
      "0              1210.0  2015-04-04      USD    1.0000  \n",
      "1              1210.0  2015-04-04      CAD    1.3884  \n",
      "2              1210.0  2015-04-04      AUD    1.3683  \n",
      "3              1210.0  2015-04-04      EUR    0.9185  \n",
      "4              1210.0  2015-04-04      GBP    0.6742  \n",
      "...               ...         ...      ...       ...  \n",
      "314415            0.0  2010-01-01      USD    1.0000  \n",
      "314416            0.0  2010-01-01      CAD    1.2610  \n",
      "314417            0.0  2010-01-01      AUD    1.2723  \n",
      "314418            0.0  2010-01-01      EUR    0.8238  \n",
      "314419            0.0  2010-01-01      GBP    0.7126  \n",
      "\n",
      "[314420 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load cleaned datasets\n",
    "customers = pd.read_csv(\"Cleaned_Customers.csv\")\n",
    "exchange_rates = pd.read_csv(\"Cleaned_Exchange_Rates.csv\")\n",
    "products = pd.read_csv(\"Cleaned_Products.csv\")\n",
    "sales = pd.read_csv(\"Cleaned_Sales.csv\")\n",
    "stores = pd.read_csv(\"Cleaned_Stores.csv\")\n",
    "\n",
    "# Convert necessary columns to datetime format and format them for MySQL (YYYY-MM-DD)\n",
    "sales[\"Order_Date\"] = pd.to_datetime(sales[\"Order_Date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "exchange_rates[\"Date\"] = pd.to_datetime(exchange_rates[\"Date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Merge datasets\n",
    "df = (\n",
    "    sales\n",
    "    .merge(customers, on=\"CustomerKey\", how=\"inner\")\n",
    "    .merge(products, on=\"ProductKey\", how=\"inner\")\n",
    "    .merge(stores, on=\"StoreKey\", how=\"inner\")\n",
    ")\n",
    "\n",
    "# Merge with exchange rates (left join to keep all sales records)\n",
    "merge_keys = [\"Order_Date\"]\n",
    "if \"Currency\" in df.columns and \"Currency\" in exchange_rates.columns:\n",
    "    merge_keys.append(\"Currency\")\n",
    "\n",
    "df = df.merge(exchange_rates, left_on=merge_keys, right_on=[\"Date\"] + merge_keys[1:], how=\"left\")\n",
    "\n",
    "# Clean up columns\n",
    "df.drop(columns=[\"State_y\", \"Country_y\", \"Date\"], inplace=True, errors=\"ignore\")\n",
    "df.rename(columns={\"State_x\": \"State\", \"Country_x\": \"Country\"}, inplace=True)\n",
    "\n",
    "# **Replace problematic values with None (MySQL NULL)**\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        df[col] = df[col].apply(lambda x: None if pd.isna(x) or x in [\"nan\", \"NaT\", \"\"] else x.strip() if isinstance(x, str) else x)\n",
    "    else:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")  # Convert invalid numeric values to NaN\n",
    "df = df.where(pd.notna(df), None)  # Convert NaN to None\n",
    "\n",
    "# **Rename columns to replace spaces with underscores**\n",
    "df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "# **Remove duplicates**\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save and display results\n",
    "df.to_csv(\"Merged_Dataset.csv\", index=False)\n",
    "print(\"Merged dataset saved as 'Merged_Dataset.csv'\")\n",
    "print(df.head())\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Display all columns for better readability\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b18accf-7997-4f6d-a2c1-241b71c17701",
   "metadata": {},
   "source": [
    "# DATABASE AND TABLES CREATION AND INSERTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64470bb2-1356-497e-bd17-4f96e7285270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-9.2.0-cp312-cp312-win_amd64.whl.metadata (6.2 kB)\n",
      "Downloading mysql_connector_python-9.2.0-cp312-cp312-win_amd64.whl (16.1 MB)\n",
      "   ---------------------------------------- 0.0/16.1 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 3.1/16.1 MB 20.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 6.0/16.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.7/16.1 MB 15.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.3/16.1 MB 14.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.9/16.1 MB 14.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.0/16.1 MB 14.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.1/16.1 MB 13.2 MB/s eta 0:00:00\n",
      "Installing collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-9.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mysql-connector-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41a3ac7a-1a46-460b-a46e-7702145a1d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database and tables created successfully.\n",
      "Processing Cleaned_Customers.csv → Customers...\n",
      "Inserting chunk 1 into Customers...\n",
      "Inserting chunk 2 into Customers...\n",
      "Inserting chunk 3 into Customers...\n",
      "Inserting chunk 4 into Customers...\n",
      "Inserting chunk 5 into Customers...\n",
      "Inserting chunk 6 into Customers...\n",
      "Inserting chunk 7 into Customers...\n",
      "Inserting chunk 8 into Customers...\n",
      "Inserting chunk 9 into Customers...\n",
      "Inserting chunk 10 into Customers...\n",
      "Inserting chunk 11 into Customers...\n",
      "Inserting chunk 12 into Customers...\n",
      "Inserting chunk 13 into Customers...\n",
      "Inserting chunk 14 into Customers...\n",
      "Inserting chunk 15 into Customers...\n",
      "Inserting chunk 16 into Customers...\n",
      "Inserting chunk 17 into Customers...\n",
      "Inserting chunk 18 into Customers...\n",
      "Inserting chunk 19 into Customers...\n",
      "Inserting chunk 20 into Customers...\n",
      "Inserting chunk 21 into Customers...\n",
      "Inserting chunk 22 into Customers...\n",
      "Inserting chunk 23 into Customers...\n",
      "Inserting chunk 24 into Customers...\n",
      "Inserting chunk 25 into Customers...\n",
      "Inserting chunk 26 into Customers...\n",
      "Inserting chunk 27 into Customers...\n",
      "Inserting chunk 28 into Customers...\n",
      "Inserting chunk 29 into Customers...\n",
      "Inserting chunk 30 into Customers...\n",
      "Inserting chunk 31 into Customers...\n",
      "Customers inserted successfully.\n",
      "Processing Cleaned_Exchange_Rates.csv → Exchange_Rates...\n",
      "Inserting chunk 1 into Exchange_Rates...\n",
      "Inserting chunk 2 into Exchange_Rates...\n",
      "Inserting chunk 3 into Exchange_Rates...\n",
      "Inserting chunk 4 into Exchange_Rates...\n",
      "Inserting chunk 5 into Exchange_Rates...\n",
      "Inserting chunk 6 into Exchange_Rates...\n",
      "Inserting chunk 7 into Exchange_Rates...\n",
      "Inserting chunk 8 into Exchange_Rates...\n",
      "Inserting chunk 9 into Exchange_Rates...\n",
      "Inserting chunk 10 into Exchange_Rates...\n",
      "Inserting chunk 11 into Exchange_Rates...\n",
      "Inserting chunk 12 into Exchange_Rates...\n",
      "Inserting chunk 13 into Exchange_Rates...\n",
      "Inserting chunk 14 into Exchange_Rates...\n",
      "Inserting chunk 15 into Exchange_Rates...\n",
      "Inserting chunk 16 into Exchange_Rates...\n",
      "Inserting chunk 17 into Exchange_Rates...\n",
      "Inserting chunk 18 into Exchange_Rates...\n",
      "Inserting chunk 19 into Exchange_Rates...\n",
      "Inserting chunk 20 into Exchange_Rates...\n",
      "Inserting chunk 21 into Exchange_Rates...\n",
      "Inserting chunk 22 into Exchange_Rates...\n",
      "Inserting chunk 23 into Exchange_Rates...\n",
      "Exchange_Rates inserted successfully.\n",
      "Processing Cleaned_Products.csv → Products...\n",
      "Inserting chunk 1 into Products...\n",
      "Inserting chunk 2 into Products...\n",
      "Inserting chunk 3 into Products...\n",
      "Inserting chunk 4 into Products...\n",
      "Inserting chunk 5 into Products...\n",
      "Inserting chunk 6 into Products...\n",
      "Products inserted successfully.\n",
      "Processing Cleaned_Sales.csv → Sales...\n",
      "Inserting chunk 1 into Sales...\n",
      "Inserting chunk 2 into Sales...\n",
      "Inserting chunk 3 into Sales...\n",
      "Inserting chunk 4 into Sales...\n",
      "Inserting chunk 5 into Sales...\n",
      "Inserting chunk 6 into Sales...\n",
      "Inserting chunk 7 into Sales...\n",
      "Inserting chunk 8 into Sales...\n",
      "Inserting chunk 9 into Sales...\n",
      "Inserting chunk 10 into Sales...\n",
      "Inserting chunk 11 into Sales...\n",
      "Inserting chunk 12 into Sales...\n",
      "Inserting chunk 13 into Sales...\n",
      "Inserting chunk 14 into Sales...\n",
      "Inserting chunk 15 into Sales...\n",
      "Inserting chunk 16 into Sales...\n",
      "Inserting chunk 17 into Sales...\n",
      "Inserting chunk 18 into Sales...\n",
      "Inserting chunk 19 into Sales...\n",
      "Inserting chunk 20 into Sales...\n",
      "Inserting chunk 21 into Sales...\n",
      "Inserting chunk 22 into Sales...\n",
      "Inserting chunk 23 into Sales...\n",
      "Inserting chunk 24 into Sales...\n",
      "Inserting chunk 25 into Sales...\n",
      "Inserting chunk 26 into Sales...\n",
      "Inserting chunk 27 into Sales...\n",
      "Inserting chunk 28 into Sales...\n",
      "Inserting chunk 29 into Sales...\n",
      "Inserting chunk 30 into Sales...\n",
      "Inserting chunk 31 into Sales...\n",
      "Inserting chunk 32 into Sales...\n",
      "Inserting chunk 33 into Sales...\n",
      "Inserting chunk 34 into Sales...\n",
      "Inserting chunk 35 into Sales...\n",
      "Inserting chunk 36 into Sales...\n",
      "Inserting chunk 37 into Sales...\n",
      "Inserting chunk 38 into Sales...\n",
      "Inserting chunk 39 into Sales...\n",
      "Inserting chunk 40 into Sales...\n",
      "Inserting chunk 41 into Sales...\n",
      "Inserting chunk 42 into Sales...\n",
      "Inserting chunk 43 into Sales...\n",
      "Inserting chunk 44 into Sales...\n",
      "Inserting chunk 45 into Sales...\n",
      "Inserting chunk 46 into Sales...\n",
      "Inserting chunk 47 into Sales...\n",
      "Inserting chunk 48 into Sales...\n",
      "Inserting chunk 49 into Sales...\n",
      "Inserting chunk 50 into Sales...\n",
      "Inserting chunk 51 into Sales...\n",
      "Inserting chunk 52 into Sales...\n",
      "Inserting chunk 53 into Sales...\n",
      "Inserting chunk 54 into Sales...\n",
      "Inserting chunk 55 into Sales...\n",
      "Inserting chunk 56 into Sales...\n",
      "Inserting chunk 57 into Sales...\n",
      "Inserting chunk 58 into Sales...\n",
      "Inserting chunk 59 into Sales...\n",
      "Inserting chunk 60 into Sales...\n",
      "Inserting chunk 61 into Sales...\n",
      "Inserting chunk 62 into Sales...\n",
      "Inserting chunk 63 into Sales...\n",
      "Inserting chunk 64 into Sales...\n",
      "Inserting chunk 65 into Sales...\n",
      "Inserting chunk 66 into Sales...\n",
      "Inserting chunk 67 into Sales...\n",
      "Inserting chunk 68 into Sales...\n",
      "Inserting chunk 69 into Sales...\n",
      "Inserting chunk 70 into Sales...\n",
      "Inserting chunk 71 into Sales...\n",
      "Inserting chunk 72 into Sales...\n",
      "Inserting chunk 73 into Sales...\n",
      "Inserting chunk 74 into Sales...\n",
      "Inserting chunk 75 into Sales...\n",
      "Inserting chunk 76 into Sales...\n",
      "Inserting chunk 77 into Sales...\n",
      "Inserting chunk 78 into Sales...\n",
      "Inserting chunk 79 into Sales...\n",
      "Inserting chunk 80 into Sales...\n",
      "Inserting chunk 81 into Sales...\n",
      "Inserting chunk 82 into Sales...\n",
      "Inserting chunk 83 into Sales...\n",
      "Inserting chunk 84 into Sales...\n",
      "Inserting chunk 85 into Sales...\n",
      "Inserting chunk 86 into Sales...\n",
      "Inserting chunk 87 into Sales...\n",
      "Inserting chunk 88 into Sales...\n",
      "Inserting chunk 89 into Sales...\n",
      "Inserting chunk 90 into Sales...\n",
      "Inserting chunk 91 into Sales...\n",
      "Inserting chunk 92 into Sales...\n",
      "Inserting chunk 93 into Sales...\n",
      "Inserting chunk 94 into Sales...\n",
      "Inserting chunk 95 into Sales...\n",
      "Inserting chunk 96 into Sales...\n",
      "Inserting chunk 97 into Sales...\n",
      "Inserting chunk 98 into Sales...\n",
      "Inserting chunk 99 into Sales...\n",
      "Inserting chunk 100 into Sales...\n",
      "Inserting chunk 101 into Sales...\n",
      "Inserting chunk 102 into Sales...\n",
      "Inserting chunk 103 into Sales...\n",
      "Inserting chunk 104 into Sales...\n",
      "Inserting chunk 105 into Sales...\n",
      "Inserting chunk 106 into Sales...\n",
      "Inserting chunk 107 into Sales...\n",
      "Inserting chunk 108 into Sales...\n",
      "Inserting chunk 109 into Sales...\n",
      "Inserting chunk 110 into Sales...\n",
      "Inserting chunk 111 into Sales...\n",
      "Inserting chunk 112 into Sales...\n",
      "Inserting chunk 113 into Sales...\n",
      "Inserting chunk 114 into Sales...\n",
      "Inserting chunk 115 into Sales...\n",
      "Inserting chunk 116 into Sales...\n",
      "Inserting chunk 117 into Sales...\n",
      "Inserting chunk 118 into Sales...\n",
      "Inserting chunk 119 into Sales...\n",
      "Inserting chunk 120 into Sales...\n",
      "Inserting chunk 121 into Sales...\n",
      "Inserting chunk 122 into Sales...\n",
      "Inserting chunk 123 into Sales...\n",
      "Inserting chunk 124 into Sales...\n",
      "Inserting chunk 125 into Sales...\n",
      "Inserting chunk 126 into Sales...\n",
      "Sales inserted successfully.\n",
      "Processing Cleaned_Stores.csv → Stores...\n",
      "Inserting chunk 1 into Stores...\n",
      "Stores inserted successfully.\n",
      "Processing Merged_Dataset.csv → Merged_Dataset...\n",
      "Inserting chunk 1 into Merged_Dataset...\n",
      "Inserting chunk 2 into Merged_Dataset...\n",
      "Inserting chunk 3 into Merged_Dataset...\n",
      "Inserting chunk 4 into Merged_Dataset...\n",
      "Inserting chunk 5 into Merged_Dataset...\n",
      "Inserting chunk 6 into Merged_Dataset...\n",
      "Inserting chunk 7 into Merged_Dataset...\n",
      "Inserting chunk 8 into Merged_Dataset...\n",
      "Inserting chunk 9 into Merged_Dataset...\n",
      "Inserting chunk 10 into Merged_Dataset...\n",
      "Inserting chunk 11 into Merged_Dataset...\n",
      "Inserting chunk 12 into Merged_Dataset...\n",
      "Inserting chunk 13 into Merged_Dataset...\n",
      "Inserting chunk 14 into Merged_Dataset...\n",
      "Inserting chunk 15 into Merged_Dataset...\n",
      "Inserting chunk 16 into Merged_Dataset...\n",
      "Inserting chunk 17 into Merged_Dataset...\n",
      "Inserting chunk 18 into Merged_Dataset...\n",
      "Inserting chunk 19 into Merged_Dataset...\n",
      "Inserting chunk 20 into Merged_Dataset...\n",
      "Inserting chunk 21 into Merged_Dataset...\n",
      "Inserting chunk 22 into Merged_Dataset...\n",
      "Inserting chunk 23 into Merged_Dataset...\n",
      "Inserting chunk 24 into Merged_Dataset...\n",
      "Inserting chunk 25 into Merged_Dataset...\n",
      "Inserting chunk 26 into Merged_Dataset...\n",
      "Inserting chunk 27 into Merged_Dataset...\n",
      "Inserting chunk 28 into Merged_Dataset...\n",
      "Inserting chunk 29 into Merged_Dataset...\n",
      "Inserting chunk 30 into Merged_Dataset...\n",
      "Inserting chunk 31 into Merged_Dataset...\n",
      "Inserting chunk 32 into Merged_Dataset...\n",
      "Inserting chunk 33 into Merged_Dataset...\n",
      "Inserting chunk 34 into Merged_Dataset...\n",
      "Inserting chunk 35 into Merged_Dataset...\n",
      "Inserting chunk 36 into Merged_Dataset...\n",
      "Inserting chunk 37 into Merged_Dataset...\n",
      "Inserting chunk 38 into Merged_Dataset...\n",
      "Inserting chunk 39 into Merged_Dataset...\n",
      "Inserting chunk 40 into Merged_Dataset...\n",
      "Inserting chunk 41 into Merged_Dataset...\n",
      "Inserting chunk 42 into Merged_Dataset...\n",
      "Inserting chunk 43 into Merged_Dataset...\n",
      "Inserting chunk 44 into Merged_Dataset...\n",
      "Inserting chunk 45 into Merged_Dataset...\n",
      "Inserting chunk 46 into Merged_Dataset...\n",
      "Inserting chunk 47 into Merged_Dataset...\n",
      "Inserting chunk 48 into Merged_Dataset...\n",
      "Inserting chunk 49 into Merged_Dataset...\n",
      "Inserting chunk 50 into Merged_Dataset...\n",
      "Inserting chunk 51 into Merged_Dataset...\n",
      "Inserting chunk 52 into Merged_Dataset...\n",
      "Inserting chunk 53 into Merged_Dataset...\n",
      "Inserting chunk 54 into Merged_Dataset...\n",
      "Inserting chunk 55 into Merged_Dataset...\n",
      "Inserting chunk 56 into Merged_Dataset...\n",
      "Inserting chunk 57 into Merged_Dataset...\n",
      "Inserting chunk 58 into Merged_Dataset...\n",
      "Inserting chunk 59 into Merged_Dataset...\n",
      "Inserting chunk 60 into Merged_Dataset...\n",
      "Inserting chunk 61 into Merged_Dataset...\n",
      "Inserting chunk 62 into Merged_Dataset...\n",
      "Inserting chunk 63 into Merged_Dataset...\n",
      "Inserting chunk 64 into Merged_Dataset...\n",
      "Inserting chunk 65 into Merged_Dataset...\n",
      "Inserting chunk 66 into Merged_Dataset...\n",
      "Inserting chunk 67 into Merged_Dataset...\n",
      "Inserting chunk 68 into Merged_Dataset...\n",
      "Inserting chunk 69 into Merged_Dataset...\n",
      "Inserting chunk 70 into Merged_Dataset...\n",
      "Inserting chunk 71 into Merged_Dataset...\n",
      "Inserting chunk 72 into Merged_Dataset...\n",
      "Inserting chunk 73 into Merged_Dataset...\n",
      "Inserting chunk 74 into Merged_Dataset...\n",
      "Inserting chunk 75 into Merged_Dataset...\n",
      "Inserting chunk 76 into Merged_Dataset...\n",
      "Inserting chunk 77 into Merged_Dataset...\n",
      "Inserting chunk 78 into Merged_Dataset...\n",
      "Inserting chunk 79 into Merged_Dataset...\n",
      "Inserting chunk 80 into Merged_Dataset...\n",
      "Inserting chunk 81 into Merged_Dataset...\n",
      "Inserting chunk 82 into Merged_Dataset...\n",
      "Inserting chunk 83 into Merged_Dataset...\n",
      "Inserting chunk 84 into Merged_Dataset...\n",
      "Inserting chunk 85 into Merged_Dataset...\n",
      "Inserting chunk 86 into Merged_Dataset...\n",
      "Inserting chunk 87 into Merged_Dataset...\n",
      "Inserting chunk 88 into Merged_Dataset...\n",
      "Inserting chunk 89 into Merged_Dataset...\n",
      "Inserting chunk 90 into Merged_Dataset...\n",
      "Inserting chunk 91 into Merged_Dataset...\n",
      "Inserting chunk 92 into Merged_Dataset...\n",
      "Inserting chunk 93 into Merged_Dataset...\n",
      "Inserting chunk 94 into Merged_Dataset...\n",
      "Inserting chunk 95 into Merged_Dataset...\n",
      "Inserting chunk 96 into Merged_Dataset...\n",
      "Inserting chunk 97 into Merged_Dataset...\n",
      "Inserting chunk 98 into Merged_Dataset...\n",
      "Inserting chunk 99 into Merged_Dataset...\n",
      "Inserting chunk 100 into Merged_Dataset...\n",
      "Inserting chunk 101 into Merged_Dataset...\n",
      "Inserting chunk 102 into Merged_Dataset...\n",
      "Inserting chunk 103 into Merged_Dataset...\n",
      "Inserting chunk 104 into Merged_Dataset...\n",
      "Inserting chunk 105 into Merged_Dataset...\n",
      "Inserting chunk 106 into Merged_Dataset...\n",
      "Inserting chunk 107 into Merged_Dataset...\n",
      "Inserting chunk 108 into Merged_Dataset...\n",
      "Inserting chunk 109 into Merged_Dataset...\n",
      "Inserting chunk 110 into Merged_Dataset...\n",
      "Inserting chunk 111 into Merged_Dataset...\n",
      "Inserting chunk 112 into Merged_Dataset...\n",
      "Inserting chunk 113 into Merged_Dataset...\n",
      "Inserting chunk 114 into Merged_Dataset...\n",
      "Inserting chunk 115 into Merged_Dataset...\n",
      "Inserting chunk 116 into Merged_Dataset...\n",
      "Inserting chunk 117 into Merged_Dataset...\n",
      "Inserting chunk 118 into Merged_Dataset...\n",
      "Inserting chunk 119 into Merged_Dataset...\n",
      "Inserting chunk 120 into Merged_Dataset...\n",
      "Inserting chunk 121 into Merged_Dataset...\n",
      "Inserting chunk 122 into Merged_Dataset...\n",
      "Inserting chunk 123 into Merged_Dataset...\n",
      "Inserting chunk 124 into Merged_Dataset...\n",
      "Inserting chunk 125 into Merged_Dataset...\n",
      "Inserting chunk 126 into Merged_Dataset...\n",
      "Inserting chunk 127 into Merged_Dataset...\n",
      "Inserting chunk 128 into Merged_Dataset...\n",
      "Inserting chunk 129 into Merged_Dataset...\n",
      "Inserting chunk 130 into Merged_Dataset...\n",
      "Inserting chunk 131 into Merged_Dataset...\n",
      "Inserting chunk 132 into Merged_Dataset...\n",
      "Inserting chunk 133 into Merged_Dataset...\n",
      "Inserting chunk 134 into Merged_Dataset...\n",
      "Inserting chunk 135 into Merged_Dataset...\n",
      "Inserting chunk 136 into Merged_Dataset...\n",
      "Inserting chunk 137 into Merged_Dataset...\n",
      "Inserting chunk 138 into Merged_Dataset...\n",
      "Inserting chunk 139 into Merged_Dataset...\n",
      "Inserting chunk 140 into Merged_Dataset...\n",
      "Inserting chunk 141 into Merged_Dataset...\n",
      "Inserting chunk 142 into Merged_Dataset...\n",
      "Inserting chunk 143 into Merged_Dataset...\n",
      "Inserting chunk 144 into Merged_Dataset...\n",
      "Inserting chunk 145 into Merged_Dataset...\n",
      "Inserting chunk 146 into Merged_Dataset...\n",
      "Inserting chunk 147 into Merged_Dataset...\n",
      "Inserting chunk 148 into Merged_Dataset...\n",
      "Inserting chunk 149 into Merged_Dataset...\n",
      "Inserting chunk 150 into Merged_Dataset...\n",
      "Inserting chunk 151 into Merged_Dataset...\n",
      "Inserting chunk 152 into Merged_Dataset...\n",
      "Inserting chunk 153 into Merged_Dataset...\n",
      "Inserting chunk 154 into Merged_Dataset...\n",
      "Inserting chunk 155 into Merged_Dataset...\n",
      "Inserting chunk 156 into Merged_Dataset...\n",
      "Inserting chunk 157 into Merged_Dataset...\n",
      "Inserting chunk 158 into Merged_Dataset...\n",
      "Inserting chunk 159 into Merged_Dataset...\n",
      "Inserting chunk 160 into Merged_Dataset...\n",
      "Inserting chunk 161 into Merged_Dataset...\n",
      "Inserting chunk 162 into Merged_Dataset...\n",
      "Inserting chunk 163 into Merged_Dataset...\n",
      "Inserting chunk 164 into Merged_Dataset...\n",
      "Inserting chunk 165 into Merged_Dataset...\n",
      "Inserting chunk 166 into Merged_Dataset...\n",
      "Inserting chunk 167 into Merged_Dataset...\n",
      "Inserting chunk 168 into Merged_Dataset...\n",
      "Inserting chunk 169 into Merged_Dataset...\n",
      "Inserting chunk 170 into Merged_Dataset...\n",
      "Inserting chunk 171 into Merged_Dataset...\n",
      "Inserting chunk 172 into Merged_Dataset...\n",
      "Inserting chunk 173 into Merged_Dataset...\n",
      "Inserting chunk 174 into Merged_Dataset...\n",
      "Inserting chunk 175 into Merged_Dataset...\n",
      "Inserting chunk 176 into Merged_Dataset...\n",
      "Inserting chunk 177 into Merged_Dataset...\n",
      "Inserting chunk 178 into Merged_Dataset...\n",
      "Inserting chunk 179 into Merged_Dataset...\n",
      "Inserting chunk 180 into Merged_Dataset...\n",
      "Inserting chunk 181 into Merged_Dataset...\n",
      "Inserting chunk 182 into Merged_Dataset...\n",
      "Inserting chunk 183 into Merged_Dataset...\n",
      "Inserting chunk 184 into Merged_Dataset...\n",
      "Inserting chunk 185 into Merged_Dataset...\n",
      "Inserting chunk 186 into Merged_Dataset...\n",
      "Inserting chunk 187 into Merged_Dataset...\n",
      "Inserting chunk 188 into Merged_Dataset...\n",
      "Inserting chunk 189 into Merged_Dataset...\n",
      "Inserting chunk 190 into Merged_Dataset...\n",
      "Inserting chunk 191 into Merged_Dataset...\n",
      "Inserting chunk 192 into Merged_Dataset...\n",
      "Inserting chunk 193 into Merged_Dataset...\n",
      "Inserting chunk 194 into Merged_Dataset...\n",
      "Inserting chunk 195 into Merged_Dataset...\n",
      "Inserting chunk 196 into Merged_Dataset...\n",
      "Inserting chunk 197 into Merged_Dataset...\n",
      "Inserting chunk 198 into Merged_Dataset...\n",
      "Inserting chunk 199 into Merged_Dataset...\n",
      "Inserting chunk 200 into Merged_Dataset...\n",
      "Inserting chunk 201 into Merged_Dataset...\n",
      "Inserting chunk 202 into Merged_Dataset...\n",
      "Inserting chunk 203 into Merged_Dataset...\n",
      "Inserting chunk 204 into Merged_Dataset...\n",
      "Inserting chunk 205 into Merged_Dataset...\n",
      "Inserting chunk 206 into Merged_Dataset...\n",
      "Inserting chunk 207 into Merged_Dataset...\n",
      "Inserting chunk 208 into Merged_Dataset...\n",
      "Inserting chunk 209 into Merged_Dataset...\n",
      "Inserting chunk 210 into Merged_Dataset...\n",
      "Inserting chunk 211 into Merged_Dataset...\n",
      "Inserting chunk 212 into Merged_Dataset...\n",
      "Inserting chunk 213 into Merged_Dataset...\n",
      "Inserting chunk 214 into Merged_Dataset...\n",
      "Inserting chunk 215 into Merged_Dataset...\n",
      "Inserting chunk 216 into Merged_Dataset...\n",
      "Inserting chunk 217 into Merged_Dataset...\n",
      "Inserting chunk 218 into Merged_Dataset...\n",
      "Inserting chunk 219 into Merged_Dataset...\n",
      "Inserting chunk 220 into Merged_Dataset...\n",
      "Inserting chunk 221 into Merged_Dataset...\n",
      "Inserting chunk 222 into Merged_Dataset...\n",
      "Inserting chunk 223 into Merged_Dataset...\n",
      "Inserting chunk 224 into Merged_Dataset...\n",
      "Inserting chunk 225 into Merged_Dataset...\n",
      "Inserting chunk 226 into Merged_Dataset...\n",
      "Inserting chunk 227 into Merged_Dataset...\n",
      "Inserting chunk 228 into Merged_Dataset...\n",
      "Inserting chunk 229 into Merged_Dataset...\n",
      "Inserting chunk 230 into Merged_Dataset...\n",
      "Inserting chunk 231 into Merged_Dataset...\n",
      "Inserting chunk 232 into Merged_Dataset...\n",
      "Inserting chunk 233 into Merged_Dataset...\n",
      "Inserting chunk 234 into Merged_Dataset...\n",
      "Inserting chunk 235 into Merged_Dataset...\n",
      "Inserting chunk 236 into Merged_Dataset...\n",
      "Inserting chunk 237 into Merged_Dataset...\n",
      "Inserting chunk 238 into Merged_Dataset...\n",
      "Inserting chunk 239 into Merged_Dataset...\n",
      "Inserting chunk 240 into Merged_Dataset...\n",
      "Inserting chunk 241 into Merged_Dataset...\n",
      "Inserting chunk 242 into Merged_Dataset...\n",
      "Inserting chunk 243 into Merged_Dataset...\n",
      "Inserting chunk 244 into Merged_Dataset...\n",
      "Inserting chunk 245 into Merged_Dataset...\n",
      "Inserting chunk 246 into Merged_Dataset...\n",
      "Inserting chunk 247 into Merged_Dataset...\n",
      "Inserting chunk 248 into Merged_Dataset...\n",
      "Inserting chunk 249 into Merged_Dataset...\n",
      "Inserting chunk 250 into Merged_Dataset...\n",
      "Inserting chunk 251 into Merged_Dataset...\n",
      "Inserting chunk 252 into Merged_Dataset...\n",
      "Inserting chunk 253 into Merged_Dataset...\n",
      "Inserting chunk 254 into Merged_Dataset...\n",
      "Inserting chunk 255 into Merged_Dataset...\n",
      "Inserting chunk 256 into Merged_Dataset...\n",
      "Inserting chunk 257 into Merged_Dataset...\n",
      "Inserting chunk 258 into Merged_Dataset...\n",
      "Inserting chunk 259 into Merged_Dataset...\n",
      "Inserting chunk 260 into Merged_Dataset...\n",
      "Inserting chunk 261 into Merged_Dataset...\n",
      "Inserting chunk 262 into Merged_Dataset...\n",
      "Inserting chunk 263 into Merged_Dataset...\n",
      "Inserting chunk 264 into Merged_Dataset...\n",
      "Inserting chunk 265 into Merged_Dataset...\n",
      "Inserting chunk 266 into Merged_Dataset...\n",
      "Inserting chunk 267 into Merged_Dataset...\n",
      "Inserting chunk 268 into Merged_Dataset...\n",
      "Inserting chunk 269 into Merged_Dataset...\n",
      "Inserting chunk 270 into Merged_Dataset...\n",
      "Inserting chunk 271 into Merged_Dataset...\n",
      "Inserting chunk 272 into Merged_Dataset...\n",
      "Inserting chunk 273 into Merged_Dataset...\n",
      "Inserting chunk 274 into Merged_Dataset...\n",
      "Inserting chunk 275 into Merged_Dataset...\n",
      "Inserting chunk 276 into Merged_Dataset...\n",
      "Inserting chunk 277 into Merged_Dataset...\n",
      "Inserting chunk 278 into Merged_Dataset...\n",
      "Inserting chunk 279 into Merged_Dataset...\n",
      "Inserting chunk 280 into Merged_Dataset...\n",
      "Inserting chunk 281 into Merged_Dataset...\n",
      "Inserting chunk 282 into Merged_Dataset...\n",
      "Inserting chunk 283 into Merged_Dataset...\n",
      "Inserting chunk 284 into Merged_Dataset...\n",
      "Inserting chunk 285 into Merged_Dataset...\n",
      "Inserting chunk 286 into Merged_Dataset...\n",
      "Inserting chunk 287 into Merged_Dataset...\n",
      "Inserting chunk 288 into Merged_Dataset...\n",
      "Inserting chunk 289 into Merged_Dataset...\n",
      "Inserting chunk 290 into Merged_Dataset...\n",
      "Inserting chunk 291 into Merged_Dataset...\n",
      "Inserting chunk 292 into Merged_Dataset...\n",
      "Inserting chunk 293 into Merged_Dataset...\n",
      "Inserting chunk 294 into Merged_Dataset...\n",
      "Inserting chunk 295 into Merged_Dataset...\n",
      "Inserting chunk 296 into Merged_Dataset...\n",
      "Inserting chunk 297 into Merged_Dataset...\n",
      "Inserting chunk 298 into Merged_Dataset...\n",
      "Inserting chunk 299 into Merged_Dataset...\n",
      "Inserting chunk 300 into Merged_Dataset...\n",
      "Inserting chunk 301 into Merged_Dataset...\n",
      "Inserting chunk 302 into Merged_Dataset...\n",
      "Inserting chunk 303 into Merged_Dataset...\n",
      "Inserting chunk 304 into Merged_Dataset...\n",
      "Inserting chunk 305 into Merged_Dataset...\n",
      "Inserting chunk 306 into Merged_Dataset...\n",
      "Inserting chunk 307 into Merged_Dataset...\n",
      "Inserting chunk 308 into Merged_Dataset...\n",
      "Inserting chunk 309 into Merged_Dataset...\n",
      "Inserting chunk 310 into Merged_Dataset...\n",
      "Inserting chunk 311 into Merged_Dataset...\n",
      "Inserting chunk 312 into Merged_Dataset...\n",
      "Inserting chunk 313 into Merged_Dataset...\n",
      "Inserting chunk 314 into Merged_Dataset...\n",
      "Inserting chunk 315 into Merged_Dataset...\n",
      "Inserting chunk 316 into Merged_Dataset...\n",
      "Inserting chunk 317 into Merged_Dataset...\n",
      "Inserting chunk 318 into Merged_Dataset...\n",
      "Inserting chunk 319 into Merged_Dataset...\n",
      "Inserting chunk 320 into Merged_Dataset...\n",
      "Inserting chunk 321 into Merged_Dataset...\n",
      "Inserting chunk 322 into Merged_Dataset...\n",
      "Inserting chunk 323 into Merged_Dataset...\n",
      "Inserting chunk 324 into Merged_Dataset...\n",
      "Inserting chunk 325 into Merged_Dataset...\n",
      "Inserting chunk 326 into Merged_Dataset...\n",
      "Inserting chunk 327 into Merged_Dataset...\n",
      "Inserting chunk 328 into Merged_Dataset...\n",
      "Inserting chunk 329 into Merged_Dataset...\n",
      "Inserting chunk 330 into Merged_Dataset...\n",
      "Inserting chunk 331 into Merged_Dataset...\n",
      "Inserting chunk 332 into Merged_Dataset...\n",
      "Inserting chunk 333 into Merged_Dataset...\n",
      "Inserting chunk 334 into Merged_Dataset...\n",
      "Inserting chunk 335 into Merged_Dataset...\n",
      "Inserting chunk 336 into Merged_Dataset...\n",
      "Inserting chunk 337 into Merged_Dataset...\n",
      "Inserting chunk 338 into Merged_Dataset...\n",
      "Inserting chunk 339 into Merged_Dataset...\n",
      "Inserting chunk 340 into Merged_Dataset...\n",
      "Inserting chunk 341 into Merged_Dataset...\n",
      "Inserting chunk 342 into Merged_Dataset...\n",
      "Inserting chunk 343 into Merged_Dataset...\n",
      "Inserting chunk 344 into Merged_Dataset...\n",
      "Inserting chunk 345 into Merged_Dataset...\n",
      "Inserting chunk 346 into Merged_Dataset...\n",
      "Inserting chunk 347 into Merged_Dataset...\n",
      "Inserting chunk 348 into Merged_Dataset...\n",
      "Inserting chunk 349 into Merged_Dataset...\n",
      "Inserting chunk 350 into Merged_Dataset...\n",
      "Inserting chunk 351 into Merged_Dataset...\n",
      "Inserting chunk 352 into Merged_Dataset...\n",
      "Inserting chunk 353 into Merged_Dataset...\n",
      "Inserting chunk 354 into Merged_Dataset...\n",
      "Inserting chunk 355 into Merged_Dataset...\n",
      "Inserting chunk 356 into Merged_Dataset...\n",
      "Inserting chunk 357 into Merged_Dataset...\n",
      "Inserting chunk 358 into Merged_Dataset...\n",
      "Inserting chunk 359 into Merged_Dataset...\n",
      "Inserting chunk 360 into Merged_Dataset...\n",
      "Inserting chunk 361 into Merged_Dataset...\n",
      "Inserting chunk 362 into Merged_Dataset...\n",
      "Inserting chunk 363 into Merged_Dataset...\n",
      "Inserting chunk 364 into Merged_Dataset...\n",
      "Inserting chunk 365 into Merged_Dataset...\n",
      "Inserting chunk 366 into Merged_Dataset...\n",
      "Inserting chunk 367 into Merged_Dataset...\n",
      "Inserting chunk 368 into Merged_Dataset...\n",
      "Inserting chunk 369 into Merged_Dataset...\n",
      "Inserting chunk 370 into Merged_Dataset...\n",
      "Inserting chunk 371 into Merged_Dataset...\n",
      "Inserting chunk 372 into Merged_Dataset...\n",
      "Inserting chunk 373 into Merged_Dataset...\n",
      "Inserting chunk 374 into Merged_Dataset...\n",
      "Inserting chunk 375 into Merged_Dataset...\n",
      "Inserting chunk 376 into Merged_Dataset...\n",
      "Inserting chunk 377 into Merged_Dataset...\n",
      "Inserting chunk 378 into Merged_Dataset...\n",
      "Inserting chunk 379 into Merged_Dataset...\n",
      "Inserting chunk 380 into Merged_Dataset...\n",
      "Inserting chunk 381 into Merged_Dataset...\n",
      "Inserting chunk 382 into Merged_Dataset...\n",
      "Inserting chunk 383 into Merged_Dataset...\n",
      "Inserting chunk 384 into Merged_Dataset...\n",
      "Inserting chunk 385 into Merged_Dataset...\n",
      "Inserting chunk 386 into Merged_Dataset...\n",
      "Inserting chunk 387 into Merged_Dataset...\n",
      "Inserting chunk 388 into Merged_Dataset...\n",
      "Inserting chunk 389 into Merged_Dataset...\n",
      "Inserting chunk 390 into Merged_Dataset...\n",
      "Inserting chunk 391 into Merged_Dataset...\n",
      "Inserting chunk 392 into Merged_Dataset...\n",
      "Inserting chunk 393 into Merged_Dataset...\n",
      "Inserting chunk 394 into Merged_Dataset...\n",
      "Inserting chunk 395 into Merged_Dataset...\n",
      "Inserting chunk 396 into Merged_Dataset...\n",
      "Inserting chunk 397 into Merged_Dataset...\n",
      "Inserting chunk 398 into Merged_Dataset...\n",
      "Inserting chunk 399 into Merged_Dataset...\n",
      "Inserting chunk 400 into Merged_Dataset...\n",
      "Inserting chunk 401 into Merged_Dataset...\n",
      "Inserting chunk 402 into Merged_Dataset...\n",
      "Inserting chunk 403 into Merged_Dataset...\n",
      "Inserting chunk 404 into Merged_Dataset...\n",
      "Inserting chunk 405 into Merged_Dataset...\n",
      "Inserting chunk 406 into Merged_Dataset...\n",
      "Inserting chunk 407 into Merged_Dataset...\n",
      "Inserting chunk 408 into Merged_Dataset...\n",
      "Inserting chunk 409 into Merged_Dataset...\n",
      "Inserting chunk 410 into Merged_Dataset...\n",
      "Inserting chunk 411 into Merged_Dataset...\n",
      "Inserting chunk 412 into Merged_Dataset...\n",
      "Inserting chunk 413 into Merged_Dataset...\n",
      "Inserting chunk 414 into Merged_Dataset...\n",
      "Inserting chunk 415 into Merged_Dataset...\n",
      "Inserting chunk 416 into Merged_Dataset...\n",
      "Inserting chunk 417 into Merged_Dataset...\n",
      "Inserting chunk 418 into Merged_Dataset...\n",
      "Inserting chunk 419 into Merged_Dataset...\n",
      "Inserting chunk 420 into Merged_Dataset...\n",
      "Inserting chunk 421 into Merged_Dataset...\n",
      "Inserting chunk 422 into Merged_Dataset...\n",
      "Inserting chunk 423 into Merged_Dataset...\n",
      "Inserting chunk 424 into Merged_Dataset...\n",
      "Inserting chunk 425 into Merged_Dataset...\n",
      "Inserting chunk 426 into Merged_Dataset...\n",
      "Inserting chunk 427 into Merged_Dataset...\n",
      "Inserting chunk 428 into Merged_Dataset...\n",
      "Inserting chunk 429 into Merged_Dataset...\n",
      "Inserting chunk 430 into Merged_Dataset...\n",
      "Inserting chunk 431 into Merged_Dataset...\n",
      "Inserting chunk 432 into Merged_Dataset...\n",
      "Inserting chunk 433 into Merged_Dataset...\n",
      "Inserting chunk 434 into Merged_Dataset...\n",
      "Inserting chunk 435 into Merged_Dataset...\n",
      "Inserting chunk 436 into Merged_Dataset...\n",
      "Inserting chunk 437 into Merged_Dataset...\n",
      "Inserting chunk 438 into Merged_Dataset...\n",
      "Inserting chunk 439 into Merged_Dataset...\n",
      "Inserting chunk 440 into Merged_Dataset...\n",
      "Inserting chunk 441 into Merged_Dataset...\n",
      "Inserting chunk 442 into Merged_Dataset...\n",
      "Inserting chunk 443 into Merged_Dataset...\n",
      "Inserting chunk 444 into Merged_Dataset...\n",
      "Inserting chunk 445 into Merged_Dataset...\n",
      "Inserting chunk 446 into Merged_Dataset...\n",
      "Inserting chunk 447 into Merged_Dataset...\n",
      "Inserting chunk 448 into Merged_Dataset...\n",
      "Inserting chunk 449 into Merged_Dataset...\n",
      "Inserting chunk 450 into Merged_Dataset...\n",
      "Inserting chunk 451 into Merged_Dataset...\n",
      "Inserting chunk 452 into Merged_Dataset...\n",
      "Inserting chunk 453 into Merged_Dataset...\n",
      "Inserting chunk 454 into Merged_Dataset...\n",
      "Inserting chunk 455 into Merged_Dataset...\n",
      "Inserting chunk 456 into Merged_Dataset...\n",
      "Inserting chunk 457 into Merged_Dataset...\n",
      "Inserting chunk 458 into Merged_Dataset...\n",
      "Inserting chunk 459 into Merged_Dataset...\n",
      "Inserting chunk 460 into Merged_Dataset...\n",
      "Inserting chunk 461 into Merged_Dataset...\n",
      "Inserting chunk 462 into Merged_Dataset...\n",
      "Inserting chunk 463 into Merged_Dataset...\n",
      "Inserting chunk 464 into Merged_Dataset...\n",
      "Inserting chunk 465 into Merged_Dataset...\n",
      "Inserting chunk 466 into Merged_Dataset...\n",
      "Inserting chunk 467 into Merged_Dataset...\n",
      "Inserting chunk 468 into Merged_Dataset...\n",
      "Inserting chunk 469 into Merged_Dataset...\n",
      "Inserting chunk 470 into Merged_Dataset...\n",
      "Inserting chunk 471 into Merged_Dataset...\n",
      "Inserting chunk 472 into Merged_Dataset...\n",
      "Inserting chunk 473 into Merged_Dataset...\n",
      "Inserting chunk 474 into Merged_Dataset...\n",
      "Inserting chunk 475 into Merged_Dataset...\n",
      "Inserting chunk 476 into Merged_Dataset...\n",
      "Inserting chunk 477 into Merged_Dataset...\n",
      "Inserting chunk 478 into Merged_Dataset...\n",
      "Inserting chunk 479 into Merged_Dataset...\n",
      "Inserting chunk 480 into Merged_Dataset...\n",
      "Inserting chunk 481 into Merged_Dataset...\n",
      "Inserting chunk 482 into Merged_Dataset...\n",
      "Inserting chunk 483 into Merged_Dataset...\n",
      "Inserting chunk 484 into Merged_Dataset...\n",
      "Inserting chunk 485 into Merged_Dataset...\n",
      "Inserting chunk 486 into Merged_Dataset...\n",
      "Inserting chunk 487 into Merged_Dataset...\n",
      "Inserting chunk 488 into Merged_Dataset...\n",
      "Inserting chunk 489 into Merged_Dataset...\n",
      "Inserting chunk 490 into Merged_Dataset...\n",
      "Inserting chunk 491 into Merged_Dataset...\n",
      "Inserting chunk 492 into Merged_Dataset...\n",
      "Inserting chunk 493 into Merged_Dataset...\n",
      "Inserting chunk 494 into Merged_Dataset...\n",
      "Inserting chunk 495 into Merged_Dataset...\n",
      "Inserting chunk 496 into Merged_Dataset...\n",
      "Inserting chunk 497 into Merged_Dataset...\n",
      "Inserting chunk 498 into Merged_Dataset...\n",
      "Inserting chunk 499 into Merged_Dataset...\n",
      "Inserting chunk 500 into Merged_Dataset...\n",
      "Inserting chunk 501 into Merged_Dataset...\n",
      "Inserting chunk 502 into Merged_Dataset...\n",
      "Inserting chunk 503 into Merged_Dataset...\n",
      "Inserting chunk 504 into Merged_Dataset...\n",
      "Inserting chunk 505 into Merged_Dataset...\n",
      "Inserting chunk 506 into Merged_Dataset...\n",
      "Inserting chunk 507 into Merged_Dataset...\n",
      "Inserting chunk 508 into Merged_Dataset...\n",
      "Inserting chunk 509 into Merged_Dataset...\n",
      "Inserting chunk 510 into Merged_Dataset...\n",
      "Inserting chunk 511 into Merged_Dataset...\n",
      "Inserting chunk 512 into Merged_Dataset...\n",
      "Inserting chunk 513 into Merged_Dataset...\n",
      "Inserting chunk 514 into Merged_Dataset...\n",
      "Inserting chunk 515 into Merged_Dataset...\n",
      "Inserting chunk 516 into Merged_Dataset...\n",
      "Inserting chunk 517 into Merged_Dataset...\n",
      "Inserting chunk 518 into Merged_Dataset...\n",
      "Inserting chunk 519 into Merged_Dataset...\n",
      "Inserting chunk 520 into Merged_Dataset...\n",
      "Inserting chunk 521 into Merged_Dataset...\n",
      "Inserting chunk 522 into Merged_Dataset...\n",
      "Inserting chunk 523 into Merged_Dataset...\n",
      "Inserting chunk 524 into Merged_Dataset...\n",
      "Inserting chunk 525 into Merged_Dataset...\n",
      "Inserting chunk 526 into Merged_Dataset...\n",
      "Inserting chunk 527 into Merged_Dataset...\n",
      "Inserting chunk 528 into Merged_Dataset...\n",
      "Inserting chunk 529 into Merged_Dataset...\n",
      "Inserting chunk 530 into Merged_Dataset...\n",
      "Inserting chunk 531 into Merged_Dataset...\n",
      "Inserting chunk 532 into Merged_Dataset...\n",
      "Inserting chunk 533 into Merged_Dataset...\n",
      "Inserting chunk 534 into Merged_Dataset...\n",
      "Inserting chunk 535 into Merged_Dataset...\n",
      "Inserting chunk 536 into Merged_Dataset...\n",
      "Inserting chunk 537 into Merged_Dataset...\n",
      "Inserting chunk 538 into Merged_Dataset...\n",
      "Inserting chunk 539 into Merged_Dataset...\n",
      "Inserting chunk 540 into Merged_Dataset...\n",
      "Inserting chunk 541 into Merged_Dataset...\n",
      "Inserting chunk 542 into Merged_Dataset...\n",
      "Inserting chunk 543 into Merged_Dataset...\n",
      "Inserting chunk 544 into Merged_Dataset...\n",
      "Inserting chunk 545 into Merged_Dataset...\n",
      "Inserting chunk 546 into Merged_Dataset...\n",
      "Inserting chunk 547 into Merged_Dataset...\n",
      "Inserting chunk 548 into Merged_Dataset...\n",
      "Inserting chunk 549 into Merged_Dataset...\n",
      "Inserting chunk 550 into Merged_Dataset...\n",
      "Inserting chunk 551 into Merged_Dataset...\n",
      "Inserting chunk 552 into Merged_Dataset...\n",
      "Inserting chunk 553 into Merged_Dataset...\n",
      "Inserting chunk 554 into Merged_Dataset...\n",
      "Inserting chunk 555 into Merged_Dataset...\n",
      "Inserting chunk 556 into Merged_Dataset...\n",
      "Inserting chunk 557 into Merged_Dataset...\n",
      "Inserting chunk 558 into Merged_Dataset...\n",
      "Inserting chunk 559 into Merged_Dataset...\n",
      "Inserting chunk 560 into Merged_Dataset...\n",
      "Inserting chunk 561 into Merged_Dataset...\n",
      "Inserting chunk 562 into Merged_Dataset...\n",
      "Inserting chunk 563 into Merged_Dataset...\n",
      "Inserting chunk 564 into Merged_Dataset...\n",
      "Inserting chunk 565 into Merged_Dataset...\n",
      "Inserting chunk 566 into Merged_Dataset...\n",
      "Inserting chunk 567 into Merged_Dataset...\n",
      "Inserting chunk 568 into Merged_Dataset...\n",
      "Inserting chunk 569 into Merged_Dataset...\n",
      "Inserting chunk 570 into Merged_Dataset...\n",
      "Inserting chunk 571 into Merged_Dataset...\n",
      "Inserting chunk 572 into Merged_Dataset...\n",
      "Inserting chunk 573 into Merged_Dataset...\n",
      "Inserting chunk 574 into Merged_Dataset...\n",
      "Inserting chunk 575 into Merged_Dataset...\n",
      "Inserting chunk 576 into Merged_Dataset...\n",
      "Inserting chunk 577 into Merged_Dataset...\n",
      "Inserting chunk 578 into Merged_Dataset...\n",
      "Inserting chunk 579 into Merged_Dataset...\n",
      "Inserting chunk 580 into Merged_Dataset...\n",
      "Inserting chunk 581 into Merged_Dataset...\n",
      "Inserting chunk 582 into Merged_Dataset...\n",
      "Inserting chunk 583 into Merged_Dataset...\n",
      "Inserting chunk 584 into Merged_Dataset...\n",
      "Inserting chunk 585 into Merged_Dataset...\n",
      "Inserting chunk 586 into Merged_Dataset...\n",
      "Inserting chunk 587 into Merged_Dataset...\n",
      "Inserting chunk 588 into Merged_Dataset...\n",
      "Inserting chunk 589 into Merged_Dataset...\n",
      "Inserting chunk 590 into Merged_Dataset...\n",
      "Inserting chunk 591 into Merged_Dataset...\n",
      "Inserting chunk 592 into Merged_Dataset...\n",
      "Inserting chunk 593 into Merged_Dataset...\n",
      "Inserting chunk 594 into Merged_Dataset...\n",
      "Inserting chunk 595 into Merged_Dataset...\n",
      "Inserting chunk 596 into Merged_Dataset...\n",
      "Inserting chunk 597 into Merged_Dataset...\n",
      "Inserting chunk 598 into Merged_Dataset...\n",
      "Inserting chunk 599 into Merged_Dataset...\n",
      "Inserting chunk 600 into Merged_Dataset...\n",
      "Inserting chunk 601 into Merged_Dataset...\n",
      "Inserting chunk 602 into Merged_Dataset...\n",
      "Inserting chunk 603 into Merged_Dataset...\n",
      "Inserting chunk 604 into Merged_Dataset...\n",
      "Inserting chunk 605 into Merged_Dataset...\n",
      "Inserting chunk 606 into Merged_Dataset...\n",
      "Inserting chunk 607 into Merged_Dataset...\n",
      "Inserting chunk 608 into Merged_Dataset...\n",
      "Inserting chunk 609 into Merged_Dataset...\n",
      "Inserting chunk 610 into Merged_Dataset...\n",
      "Inserting chunk 611 into Merged_Dataset...\n",
      "Inserting chunk 612 into Merged_Dataset...\n",
      "Inserting chunk 613 into Merged_Dataset...\n",
      "Inserting chunk 614 into Merged_Dataset...\n",
      "Inserting chunk 615 into Merged_Dataset...\n",
      "Inserting chunk 616 into Merged_Dataset...\n",
      "Inserting chunk 617 into Merged_Dataset...\n",
      "Inserting chunk 618 into Merged_Dataset...\n",
      "Inserting chunk 619 into Merged_Dataset...\n",
      "Inserting chunk 620 into Merged_Dataset...\n",
      "Inserting chunk 621 into Merged_Dataset...\n",
      "Inserting chunk 622 into Merged_Dataset...\n",
      "Inserting chunk 623 into Merged_Dataset...\n",
      "Inserting chunk 624 into Merged_Dataset...\n",
      "Inserting chunk 625 into Merged_Dataset...\n",
      "Inserting chunk 626 into Merged_Dataset...\n",
      "Inserting chunk 627 into Merged_Dataset...\n",
      "Inserting chunk 628 into Merged_Dataset...\n",
      "Inserting chunk 629 into Merged_Dataset...\n",
      "Merged_Dataset inserted successfully.\n",
      "\n",
      "Tables in Data_Spark: ['customers', 'exchange_rates', 'merged_dataset', 'products', 'sales', 'stores']\n",
      "MySQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import configparser\n",
    "import pandas as pd\n",
    "\n",
    "# Read database credentials from config.ini\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")  # Ensure this file is present in the working directory\n",
    "\n",
    "host = config[\"mysql\"][\"host\"]\n",
    "user = config[\"mysql\"][\"user\"]\n",
    "password = config[\"mysql\"][\"password\"]\n",
    "database = \"Data_Spark\"  # Fixed database name as required\n",
    "\n",
    "# Connect to MySQL server (without specifying database to create it first)\n",
    "connection = mysql.connector.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password\n",
    ")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Create Database\n",
    "cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {database};\")\n",
    "cursor.execute(f\"USE {database};\")\n",
    "\n",
    "# **Table Creation Queries**\n",
    "tables = {\n",
    "    \"Customers\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Customers (\n",
    "            CustomerKey INT PRIMARY KEY,\n",
    "            Gender VARCHAR(10),\n",
    "            Name VARCHAR(255),\n",
    "            City VARCHAR(100),\n",
    "            State VARCHAR(100),\n",
    "            Country VARCHAR(100),\n",
    "            Birthday DATE\n",
    "        );\n",
    "    \"\"\",\n",
    "    \"Exchange_Rates\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Exchange_Rates (\n",
    "            Date DATE,\n",
    "            Currency VARCHAR(10),\n",
    "            Exchange DECIMAL(10,4),\n",
    "            PRIMARY KEY (Date, Currency)\n",
    "        );\n",
    "    \"\"\",\n",
    "    \"Products\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Products (\n",
    "            ProductKey INT PRIMARY KEY,\n",
    "            Product_Name VARCHAR(255),\n",
    "            Brand VARCHAR(100),\n",
    "            Color VARCHAR(50),\n",
    "            Unit_Cost_USD DECIMAL(10,2),\n",
    "            Unit_Price_USD DECIMAL(10,2),\n",
    "            Subcategory VARCHAR(100),\n",
    "            CategoryKey INT,\n",
    "            Category VARCHAR(100)\n",
    "        );\n",
    "    \"\"\",\n",
    "    \"Sales\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Sales (\n",
    "            Order_Number INT,\n",
    "            Line_Item INT,\n",
    "            Order_Date DATE,\n",
    "            Delivery_Date DATE,\n",
    "            CustomerKey INT,\n",
    "            StoreKey INT,\n",
    "            ProductKey INT,\n",
    "            Quantity INT,\n",
    "            Currency_Code VARCHAR(10),\n",
    "            PRIMARY KEY (Order_Number, Line_Item)\n",
    "        );\n",
    "    \"\"\",\n",
    "    \"Stores\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Stores (\n",
    "            StoreKey INT PRIMARY KEY,\n",
    "            Country VARCHAR(100),\n",
    "            State VARCHAR(100),\n",
    "            Square_Meters DECIMAL(10,2),\n",
    "            Open_Date DATE\n",
    "        );\n",
    "    \"\"\",\n",
    "    \"Merged_Dataset\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Merged_Dataset (\n",
    "            CustomerKey INT,\n",
    "            Gender VARCHAR(10),\n",
    "            Name VARCHAR(255),\n",
    "            City VARCHAR(100),\n",
    "            State VARCHAR(100),\n",
    "            Country VARCHAR(100),\n",
    "            Birthday DATE,\n",
    "            Order_Number INT,\n",
    "            Line_Item INT,\n",
    "            Order_Date DATE,\n",
    "            Delivery_Date DATE,\n",
    "            StoreKey INT,\n",
    "            ProductKey INT,\n",
    "            Quantity INT,\n",
    "            Currency_Code VARCHAR(10),\n",
    "            Product_Name VARCHAR(255),\n",
    "            Brand VARCHAR(100),\n",
    "            Color VARCHAR(50),\n",
    "            Unit_Cost_USD DECIMAL(10,2),\n",
    "            Unit_Price_USD DECIMAL(10,2),\n",
    "            Subcategory VARCHAR(100),\n",
    "            CategoryKey INT,\n",
    "            Category VARCHAR(100),\n",
    "            Square_Meters DECIMAL(10,2),\n",
    "            Open_Date DATE,\n",
    "            Currency VARCHAR(10),\n",
    "            Exchange DECIMAL(10,4)\n",
    "        );\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Execute table creation queries\n",
    "for table_name, query in tables.items():\n",
    "    cursor.execute(query)\n",
    "\n",
    "connection.commit()\n",
    "print(\"✅ Database and tables created successfully.\")\n",
    "\n",
    "# **Function to Load and Insert Data in Chunks**\n",
    "def load_and_insert_data(csv_file, table_name, chunk_size=1000):\n",
    "    print(f\"Processing {csv_file} → {table_name}...\")\n",
    "    \n",
    "    # Load CSV file in chunks\n",
    "    chunk_iter = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "\n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        print(f\"Inserting chunk {i + 1} into {table_name}...\")\n",
    "\n",
    "        # **Fix column names (replace spaces with underscores)**\n",
    "        chunk.columns = chunk.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "        # **Convert NaN, NaT, empty strings to None (MySQL NULL)**\n",
    "        for col in chunk.columns:\n",
    "            if chunk[col].dtype == object:\n",
    "                chunk[col] = chunk[col].apply(lambda x: None if pd.isna(x) or x in [\"nan\", \"NaT\", \"\"] else x.strip() if isinstance(x, str) else x)\n",
    "            else:\n",
    "                chunk[col] = pd.to_numeric(chunk[col], errors=\"coerce\")\n",
    "        chunk = chunk.where(pd.notna(chunk), None)\n",
    "\n",
    "        # **Remove Duplicates**\n",
    "        chunk.drop_duplicates(inplace=True)\n",
    "\n",
    "        # **Convert DataFrame to Tuple Format**\n",
    "        data_tuples = [tuple(x) for x in chunk.itertuples(index=False, name=None)]\n",
    "\n",
    "        # **Create Insert Query**\n",
    "        columns = \", \".join(chunk.columns)\n",
    "        placeholders = \", \".join([\"%s\"] * len(chunk.columns))\n",
    "        insert_query = f\"INSERT IGNORE INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "        # **Insert Data in Chunks**\n",
    "        cursor.executemany(insert_query, data_tuples)\n",
    "        connection.commit()\n",
    "    \n",
    "    print(f\"{table_name} inserted successfully.\")\n",
    "\n",
    "# **Load and Insert Data from CSV Files in Chunks**\n",
    "csv_table_mapping = {\n",
    "    \"Cleaned_Customers.csv\": \"Customers\",\n",
    "    \"Cleaned_Exchange_Rates.csv\": \"Exchange_Rates\",\n",
    "    \"Cleaned_Products.csv\": \"Products\",\n",
    "    \"Cleaned_Sales.csv\": \"Sales\",\n",
    "    \"Cleaned_Stores.csv\": \"Stores\",\n",
    "    \"Merged_Dataset.csv\": \"Merged_Dataset\"\n",
    "}\n",
    "\n",
    "for csv_file, table_name in csv_table_mapping.items():\n",
    "    load_and_insert_data(csv_file, table_name, chunk_size=500)  # Adjust chunk_size as needed\n",
    "\n",
    "\n",
    "# **Verify Tables**\n",
    "cursor.execute(\"SHOW TABLES;\")\n",
    "tables = cursor.fetchall()\n",
    "print(\"\\nTables in Data_Spark:\", [t[0] for t in tables])\n",
    "\n",
    "# **Close Connection**\n",
    "cursor.close()\n",
    "connection.close()\n",
    "print(\"MySQL connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251bf270-b944-4214-aec3-d8421ea719aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
